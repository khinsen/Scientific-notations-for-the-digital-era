\section{Simple and flexible formal languages}
\label{simple-and-flexible}

The criteria exposed in the last section (section~\ref{HCI}) lead to a technical requirement for digital scientific notations: it must accommodate a large number of small and simple formal languages and make it straightforward to define variants of them. This may well seem impossible to many computational scientists. Large, rigid, general-purpose languages are today's standard for software development, whereas small, rigid, and undocumented languages dominate scientific data storage. However, there are examples of more flexible formal languages, which can serve as a source of inspiration for the development of digital scientific notations. I will describe two of them in this section.

The main technical obstacle to flexibility in formal languages is the requirement for composition that I have discussed earlier (section~\ref{composition-digital}): the information items that enter into a composition must all be expressed in the same language. If that condition is not satisfied, an additional effort must be invested in the form of language conversion or more complex software that can process multiple languages.

The solution is to design a \textit{framework} for a \textit{family} of formal languages, and develop generic tools that can process any member of this family and also compositions of different members. In other words, flexibility enters the design and the support infrastructure at a very early stage. This principle should become clearer from two concrete examples: XML and Lisp.

\subsection{XML: composable data formats}
\label{XML}

XML \cite{_extensible_1998} is a framework for defining formal languages that express tree-structured data. The central concept in XML is the \textit{element}, which is a node in a tree whose type is identified by a tag. The tag also defines which attributes the element can have, and which conditions its child elements must satisfy. A concrete XML-based data format is defined by a \textit{schema}, which contains an exhaustive list of the allowed tags and the constraints on the element types defined by each tag. Given a data file and the schema it is supposed to respect, generic XML processing tools can validate the data file, i.e. check that it conforms to the schema, and also perform many types of data transformation that do not depend on the semantics of the data. Finally, writing programs that do semantics-dependent processing is facilitated by support libraries that take care of the semantics-independent operations, in particular parsing and validating the incoming information and producing correct result files. Because of these advantages, XML has become very popular and a large variety of schemas has been defined. Examples that may be familiar to computational scientists are MathML and OpenMath for mathematical formulas, SVG for vector graphics, CML for chemical data, and SBML for systems biology.

Composition of XML data means constructing a tree from elements defined in different schemas. This was made possible with the introduction of XML \textit{namespaces}. A single-schema XML document starts with a reference to its schema. A multi-schema XML document lists multiple schemas and associates a unique name with each of them. That name is then prefixed to each tag in the document. This prefix ensures that even in the presence of tag homonyms in the document's schemas, each element has a unique and well-defined tag.

The XML namespace mechanism is an implementation of the superlanguage approach that I have described earlier (section~\ref{composition-digital}). Processing such superlanguages is made straightforward because the mechanisms for defining them are part of the XML definition. All modern XML processing software implements namespaces, and therefore can handle arbitrary superlanguages inside the XML universe.

XML namespaces are not a magical solution to composing unrelated data items. Any software that performs semantics-dependent processing still needs to deal with each schema individually. But the tasks of defining languages, processing them, and processing compositions are enormously simplified by the XML framework. Defining an XML schema is much simpler than designing a complete data format, let alone a data format open for extensions. Processing someone else's XML data is also much simpler than processing someone else's ad-hoc data format, because the schema provides a minimum of documentation. Finally, the namespace mechanism encourages the definition of small schemas that can then be composed, making well-designed XML-based data formats easier to understand for human readers.

\subsection{Lisp: extensible programming languages}
\label{lisp}

Most programming languages used today are constructed in much the same way. A syntax definition specifies which sequences of text characters are legal programs. This syntax definition is set in stone by the language designer. Some syntactical elements define fundamental data types, others fundamental executable operations. These basic building blocks can be combined by the programmer into larger-scale building blocks using language constructs for defining data structures, procedures, classes, etc. In fact, programming is almost synonymous with defining such entities and giving them names for later referring to them. In other words, programming means extending the language by new building blocks, the last of which is the program to be run. The programmer cannot modify the syntax in any way, nor take any features away from the basic language. This means in particular that the programmer cannot make the language any \textit{simpler}.

One of the oldest family of programming languages, the Lisp family, differs from this picture in an important way. Its syntax is defined in two stages. The first stage merely defines how a central data structure called a \textit{list} is written in terms of text characters. The elements of a list can be any basic Lisp data type, e.g. numbers or symbols, but also other lists. Nested lists are equivalent to trees, and in fact Lisp's nested lists are very similar to the trees of elements that I have described in the section on XML (section~\ref{XML}). The second stage of Lisp's syntax defines which lists are legal programs. The general convention is that the first element of a list specifies a language construct to which the remaining elements are parameters. For example, the list `(+ 2 3)` means "perform the + operation on the numbers 2 and 3", whereas the list `(define x (+ 2 3))` means "set variable x to the value of the expression defined by the list `(+ 2 3)`".

This two-stage syntax is exploited in what is a very rare feature in programming languages: the second syntax level can be modified by the programmer, using a language construct called a \textit{macro}. Technically, a macro is a function called as part of the compilation of Lisp code. When the compiler hits a list whose first element specifies a macro, it runs the macro function and substitutes the original macro-calling list by the macro function's return value, which is then compiled instead.

To understand the power of this construct, consider that a compiler is a program that transforms another program written in language A into an equivalent program written in language B. That is exactly what a macro does: it translates a program written in some language M into basic Lisp. The language M is defined by the macro itself, just like any compiler is an operational definition of a language as I explained before (section~\ref{formal-languages}). Whatever the macro accepts as arguments is a valid program in M. A macro thus \textit{is} a compiler, and by defining macros a programmer can define his or her own languages with no other restrictions than respecting the top layer of Lisp's syntax, i.e. the list syntax. Most macros merely define small variations on the basic Lisp language, but nothing stops you from writing a `fortran` macro to implement a language equivalent to Fortran except that its syntax is defined in terms of nested lists.

The use of macros as building blocks of compilers has been pushed to a very advanced level in the Racket language \cite{plt-tr1}, a Lisp dialect which its developers describe as a "programmable programming language". The path from the first Lisp macros of the 1960s via Scheme's hygienic macros to today's Racket has been a long one. For example, it turned out that making macros composable is not trivial \cite{flatt_composable_2002}. Today's Racket programming environment contains a large number of languages for various purposes. Plain "racket" is a standard general-purpose programming language. A core subset of "racket" is available as "racket/base". Several languages are simplified forms of "racket" for teaching purposes. The simplification does not merely take out language features, but exploits the gain in simplicity for providing better error messages. Other languages are extensions, such as "typed/racket" which adds static type checking. But Racket also lifts the traditional Lisp restriction of list-based syntax, providing a mechanism to write language-specific parsers. Both Java and Python have been implemented in Racket in this way. A language definition in Racket is nothing but a library \cite{tobin-hochstadt_languages_2011}, meaning that any number of languages can co-exist. Moreover, a new language can be based on any existing one, making it straightforward to define small modifications.

A big advantage of the Lisp/Racket approach to implementing new languages is that all those languages are interoperable, because they are all compiled to basic Lisp/Racket. This is an implementation of the translation approach to composing different languages that I have described before (section~\ref{composition-digital}). Another advantage is that defining new languages becomes much easier. Implementing a big language such as Python remains a difficult task even in Racket. But implementing a small variation on an existing language -- take away some parts, add some others -- is simple enough to be accessible to an average software developer.

\section{Designing digital scientific notations}
\label{design-guidelines}

The main conclusion from the analysis that I have presented in this essay is that digital scientific notations should be based on formal languages with the following properties:

 - **Small and simple**: each formal language must be so small and simple that a scientist can memorize it easily and understand its semantics in detail.

 - **Flexible**: a scientist must be able to create modifications of existing languages used in his/her field in order to adapt them to new requirements and personal preferences.

 - **Interoperable**: composition of digital knowledge items expressed in different languages must be possible with reasonable effort.

The two examples (section~\ref{simple-and-flexible}) I have presented above suggest that a good approach is to define a framework of languages and implement generic tools for common manipulations. The foundation of this framework should provide basic data types and data structures:

 - numbers (integers, rationals, floating-point, machine-level integers)
 - symbols
 - text
 - N-dimensional arrays
 - trees
 - sets
 - key-value maps (also called associative arrays, hash tables, or dictionaries)

The representation of these fundamental data types in terms of bit sequences can be based on existing standards such as XML (text) or HDF5 (binary). It is probably inevitable to have multiple such representations to take into account conflicting requirements of different application domains. As long as automatic loss-less interconversion can be ensured, this should not be an obstacle to interoperability. An added advantage of keeping the lowest level of representation flexible is the possibility to adapt to future technological developments, for example IPFS \cite{benet_ipfs_2014} whose "permanent Web" approach seems well adapted to preserving the scientific record.

There should also be a way to represent algorithms, but it is less obvious how this should best be done. Any of the common Turing-complete formalisms (lambda calculus, term rewriting, ...) could be used, but it may turn out to be useful to have access to less powerful formalisms as well, because they facilitate the automated analysis of algorithms.

A next layer could introduce domain-specific but still widely used data abstractions, e.g. from geometry. For much of mathematics, the OpenMath content dictionaries \cite{openmath_society_openmath_2000} could be adopted. On top of this layer, each scientific community can build its own digital scientific notations, and each scientist can fine-tune them to specific needs.

An illustration of how these principles can be applied is given by the MOlecular SimulAtion Interchange Conventions (MOSAIC) \cite{hinsen_mosaic_2014}, which define a digital notation for molecular simulations. MOSAIC lacks the common layer of data types listed above, and is therefore not easily interoperable with other (future) digital notations. It does, however, define data structures specific to molecular simulations in terms of more generic data structures, in particular arrays. MOSAIC defines two bit-level representations, based on XML and HDF5. A Python library \cite{hinsen_pymosaic_2014} proposes three further implementations in terms of Python data structures, and implements I/O to and from the XML and HDF5 representations.

Traditional scientific notations have evolved as a byproduct of scientific research, and digital scientific notations will have to evolve in the same way in order to be well adapted to the task. In this spirit, the ideas listed in this section are merely the basis I intend to use in my own future work, but they may well turn out to be a dead end in the long run. I would like to encourage computational scientists to develop their own approaches if they think they can do better. As I have stated in the introduction, my goal with this essay is not to propose solutions, but to expose the problem. If computational scientists start to think about "digital scientific notation" rather than "file formats" and "programming languages", I consider my goal achieved.

\bibliography{scientific_notation}
\bibliographystyle{plain}

\end{document}


\]