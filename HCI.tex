\section{Human-computer interactions through formal languages}
\label{HCI}

If computers are to be powerful tools for scientific research, the computer's user interface must be designed to make the interaction of scientists with computers fluent and error-free. Whereas most other uses of computers happen through relatively simple interfaces (forms, graphical representations, command lines, ...), the interface between a scientist and a computer includes the formal languages in which scientific information is encoded for computation. In this respect, computational science resembles software development, where the human-computer interface includes programming languages. This similarity explains why techniques and tools from software engineering are increasingly adopted by computational science.

It is widely recognized in software engineering that software source code should be written primarily to explain the working of a program to other programmers, with executability on a computer being a technical constraint in this endeavor rather its main objective. Some authors even go farther and claim that the human understanding of the program, shared by the members of its development team, is the primary output of software development, because it is what enables the team to maintain and adapt the program as requirements evolve \cite{naur_programming_1985}. Software engineering research has therefore started to investigate the usability of programming languages by programers \cite{_plateau_2010}.

In scientific research, human understanding takes an even more prominent role because developing an understanding of nature is the ultimate goal of science. Research tools, including software, are only a means to this end. Digital scientific notations are the main human-computer interface for research, and must be developed with that role in mind. The use of formal languages is a technical constraint, but suitability for research work and communication by human scientists must be the main design criterion.

Today's digital scientific notations are programming languages and more or less well-defined file formats. In this section, I will outline the lessons learned from working with these adopted notations, and the consequences we should draw for the design of proper scientific notations in the future.

\subsection{Human vs. computational semantics}
\label{HCI-semantics}

A programming language fully defines the meaning of a program, and thus completely defines the result of a computation.\footnote{At least it does in theory. The definitions of many popular languages are incomplete and ambiguous \cite{regehr_guide_2010}.} However, software source code has a second semantic layer which matters only for human readers: references to conceptual domain knowledge in the choice of identifiers. Mismatches between what a program does and what its source code suggests it does are a common source of mistakes in scientific software.

As an illustration, consider the following piece of Python code:

\begin{verbatim}
def product(numbers):
    result = 1
    for factor in numbers:
        result = result + factor
    return result
\end{verbatim}

To a human reader, the names \texttt{product}, \texttt{numbers}, and \texttt{factor} clearly suggest that this procedure multiplies a list of numbers. A careful reader would notice the \texttt{+} sign, indicating addition rather than multiplication. The careful reader would thus conclude that this is a multiplication program containing a mistake. This is exactly how a scientist reads formulas in a journal article: their meaning is inferred from the meaning of all their constituents, using an understanding of the context and an awareness of the possibility of mistakes.

For a computer, the procedure shown above simply computes the sum of a list of numbers. The identifiers carry no meaning at all; all that matters is that different identifiers refer to different things. As a consequence, the above procedure is executed without any error message.

If we analyze the situations that typically lead to program code like the above example, the careful human reader turns out to be right: most probably, the intention of the author is to perform multiplication, and the plus sign is a mistake. It is highly unlikely that the author wanted to perform an addition and chose multiplication-related terms to confuse readers.

Since the program is perfectly coherent from a formal point of view, approaches based on algorithmic source code analysis, such as type checking, cannot find such mistakes. Software testing can be of help, unless similar mistakes enter into both the application code and the test code. Of today's software engineering techniques, the ones most likely to be of help are pair programming and code review. Like peer review of scientific articles, they rely on critical inspection by other humans.

Code review is also similar to peer review in that it is reliable only if the reviewer is an expert in the domain, even more so than the original author. In the case of software, the reviewer must have a perfect understanding of the programming languages and libraries used in the project. This is not obvious from the above example, which is particularly short and simple. Any careful reader will likely spot the mistake, even without much programming experience. But more subtle mistakes of this type do happen and do go unnoticed, in particular when advanced language features are used that perhaps even the code's author does not fully understand. As an example, few scientists with basic Python knowledge are aware of the fact that the above five-line function could in fact compute almost anything at all, depending on the context in which it is used. All it takes is a class definition that defines addition in an unexpected way. \footnote{Lest more experienced Pythonistas put up a smug grin reading this, I suggest they ask themselves if they fully understand how far the code's result can be manipulated from the outside using metaclasses. I am the first to admit that I don't.}

The main conclusion to draw from this is that digital scientific knowledge must be written in terms of very simple formal languages, in order to make human reviewing effective for finding mistakes. All the semantic implications of a knowledge item must be clear from the information itself and from the definition of the formal language it is written in. Moreover, a scientist working in the same domain should be able to read, understand, and memorize the language definition with reasonable effort and ideally pick it up while acquiring the domain knowledge itself, which is how we learn most of traditional scientific notation.

\subsection{Flexibility in scientific notation}
\label{flexibility}

As I mentioned above (section~\ref{evolution}), traditional pre-digital scientific notation is the result of an evolutionary process. In principle, scientists can use whatever notation they like, on the condition that they explain it in their publication. However, there is social pressure towards using well-established notation rather than inventing new ones. In practice, this leads to variable notation for new concepts that becomes more uniform over time as the concepts are better understood and consensus is reached about representing them in writing.

In contrast, formal languages used in computing are very rigid. The reasons are numerous and include technical aspects (ease of design and implementation) as well as historical ones (the advantages of flexibility were not immediately recognized). Perhaps the biggest barrier to flexibility left today is the near universal acceptance of rigidity as normal and inevitable, in spite of the problems that result from it. Most data formats used in computational science do not permit any variation at all. When data formats turn out to be insufficient for a new use case, the two possible choices are to "bend the rules" by violating some part of the definition, or to define a new format. Since bending the rules is often the solution of least effort in the short run, many data formats become ambiguous over time, with different software packages implementing different "dialects" of what everyone pretends to be a common format. \footnote{Readers familiar with computational structural biology have probably had bad surprises of this kind with the PDB \cite{wwpdb_atomic_2011} format.} Since computer programs lack the contextual background of humans, they cannot detect such variations, leading to an erroneous interpretation of data.

Programming languages are vastly more complex than data formats. In particular, implementing a programming language by writing of a compiler or interpreter is a significant effort, and requires competences that most computational scientists do not have. As a consequence, the programming languages used for computational science are few in number. Moreover, they are under the control of the individuals, teams, or institutions that produce their implementations. For all practical purposes, computational scientists consider programming languages as imposed from outside. The only choice left to the individual scientist or team is which of the existing languages to use, and then work around its limitations.

A digital scientific notation should offer the same level of flexibility as traditional scientific notation: a scientist should be able to state "I use conventions X and Y with the following modifications", defining the modifications in a formal language to make them usable by computers. Social pressure, e.g. in peer review, would limit abuses of this flexibility and lead to consensus formation in the long run.

\subsection{References to the scientific record}
\label{sr-references}

The main infrastructure of science as a social process is the \textit{scientific record}, which consists of the totality of scientific knowledge conserved in journal articles, monographs, textbooks, and electronic databases of many kinds. Scientists refer to the scientific record when they base new studies on prior work, but also when they comment on work by their peers, or when they summarize the state of the art in a review article, a monograph, or a textbook.

In scientific narratives, references to the scientific record are often imprecise by citing only a journal article, leaving it to the reader to find the relevant part of this article. It is, however, quite possible to refer to a specific figure or equation by a number. For computational work, references must be more precise: a dataset, a number, an equation. A digital scientific notation must therefore encourage the use of small information items that can be referenced individually while at the same time keeping track of their context. It matters that composite information items can be referenced as a whole but also permit access to their individual ingredients, as I have illustrated in my celestial mechanics example (section~\ref{composition}).

The rapidly increasing volume of scientific data and facts is creating a need for computer-aided analysis of the network of scientific knowledge. This has motivated the development of \textit{semantic publishing} \cite{shotton_semantic_2009}, which consists in publishing scientific findings in a machine-readable form where concepts become references to ontologies. Current research in semantic publishing focuses on giving machine-readable semantics to non-quantitative statements that are typically transmitted by the narrative of a journal article. The development of digital scientific notations that I wish to encourage by this essay can be seen as a variant of semantic publishing applied to computational methods. In this analogy, today's scientific software is similar to today's journal articles in that neither form of expression permits the automated extraction of embedded knowledge items.
