\section{Digital scientific knowledge}
\label{digital}

In the context of computing, factual knowledge is stored in \textit{datasets}, whereas procedural knowledge takes the form of \textit{algorithms}. Conceptual knowledge is not affected by the transition from manual to mechanized computation. Like research articles and reference tables, datasets and algorithms implicitly refer to conceptual knowledge to give meaning to their contents. However, the concepts are not explicitly represented in the computer, because they are not required to perform a computation. Applying algorithms to datasets is a purely mechanical operation that does not require any knowledge or understanding of the underlying concepts. What \textit{does} require an understanding of the concepts is the verification that a given computation is scientifically meaningful.

It is of course \textit{possible} to store and process conceptual knowledge using computers, e.g. in the form of \href{http://en.wikipedia.org/wiki/Ontology_\%28information_science\%29}{ontologies}, which represent conceptual knowledge as factual knowledge at a different semantic level. Such approaches are finding their place in scientific communication in the form of \textit{semantic publishing} \cite{shotton_semantic_2009}, whose goal is to make the scientific record machine-readable and thus accessible to automated analysis. However, performing a computation and managing information \textit{about} this computation are different and independent operations, just like using a microscope is a different activity from researching the history of microscopy. I will come back to the role of digital scientific notations in semantic publishing later (section~\ref{sr-references}).

This dissociation of mechanical computation from the conceptual knowledge base that defines its meaning has been recognized as a problem in various domains of digital knowledge management, for example in database design \cite{borgida_data_2004}. The typical symptom is the existence of electronic datasets that nobody can interpret any more, because the original designers of the software and data formats did not document their work sufficiently well for their colleagues and successors. A frequent variant is people modifying software and data formats without updating the documentation. Every computational scientist has probably experienced the difficulties of dealing with datasets stored in undocumented formats, and with software whose inner workings are not described anywhere in an understandable form.

The most vicious manifestation of this problem relates to scientific software. Even when software is developed respecting the best practices of software engineering, it may nevertheless compute something else than what its users think it computes. Documentation can help to some degree by explaining the authors' intentions to the users, but nothing permits to verify that the documentation is complete and accurate. The only way to make sure that users understand what a piece of software computes is making the software's source code comprehensible for human readers. Today, most scientific software source code is unintelligible to its users, and sometimes it even becomes unintelligible to its developers over time.

Some of the problems we are observing in computational science today are direct consequences of the fact that scientists have an insufficient understanding of the software they use. In particular, it suffers from rampant software errors \cite{soergel_rampant_2014,merali_computational_2010} and the near-universal non-reproducibility of computational results \cite{stodden_setting_2013,peng_reproducible_2011}. The scientific community has failed so far to fully appreciate the double role of scientific software as tools for performing computations and as repositories of scientific knowledge \cite{hinsen_computational_2014}. It has uncritically adopted notations for digital knowledge that are not adapted to human communication. As a consequence, the all-important critical discourse that makes scientific research self-correcting in the long run does not adequately cover digital scientific knowledge.
